{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "608360c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "import cv2\n",
    "import yaml\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import albumentations as A\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from types import SimpleNamespace\n",
    "from ultralytics import YOLO\n",
    "from types import SimpleNamespace\n",
    "from ultralytics.utils.loss import v8DetectionLoss\n",
    "\n",
    "\n",
    "import math\n",
    "from transformers import TrainerCallback\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af076a5",
   "metadata": {},
   "source": [
    "## 1) Mapeamento de classes (`id2label` e `label2id`)\n",
    "\n",
    "Definindo **a “tabela de tradução”** entre:\n",
    "\n",
    "- o **ID numérico** que o modelo usa internamente (0, 1)\n",
    "- o **nome** da classe (“fruit”, “bomb”)\n",
    "\n",
    "Isso é feito pois os modelos não entendem caracteres e sim números, então passamos cada classe de forma numérica, mas no momento de nós utilizarmos fica mais visual em texto.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87ef9107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'fruit', 1: 'bomb'}\n",
      "{'fruit': 0, 'bomb': 1}\n"
     ]
    }
   ],
   "source": [
    "id2label = {\n",
    "    0: \"fruit\",\n",
    "    1: \"bomb\",\n",
    "}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43f28ea",
   "metadata": {},
   "source": [
    "## 2) `Processor`: do frame ao batch\n",
    "\n",
    "**um processador de imagem + bounding boxes** que:\n",
    "\n",
    "1. Recebe a imagem crua (BGR do OpenCV) e as caixas no formato YOLO (`x, y, w, h` normalizado).\n",
    "2. Aplica um **resize com manutenção de proporção** + **pad** para bater no `imgsz` do modelo.\n",
    "3. Se `train=True` aplica augmentations que é o processo de alteração das imagens para garantir que o modelo não irá fazer overfiting e aprender de fato as features.\n",
    "4. Retorna tudo em tensores PyTorch no formato que o `collate_fn` e o `criterion` do YOLO espera.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411b3769",
   "metadata": {},
   "source": [
    "## O que é “pad” (padding)?\n",
    "\n",
    "**Pad**/**padding** é literalmente **preencher as “bordas vazias”** de uma imagem com pixels extras para ela atingir um tamanho alvo.\n",
    "\n",
    "No uso em questão o alvo é: `640 x 640`.\n",
    "\n",
    "---\n",
    "\n",
    "## Por que você precisa de pad no YOLO?\n",
    "\n",
    "1. `LongestMaxSize(max_size=imgsz)`  \n",
    "   → redimensiona mantendo a proporção (aspect ratio)\n",
    "\n",
    "2. `PadIfNeeded(min_height=imgsz, min_width=imgsz)`  \n",
    "   → completa o que faltar até virar um quadrado `imgsz x imgsz`\n",
    "\n",
    "Exemplo prático:\n",
    "\n",
    "- Frame original: **1280 x 720**\n",
    "- Após `LongestMaxSize(640)`: **640 x 360** (mantém proporção)\n",
    "- Só que o modelo quer **640 x 640**\n",
    "- Então entra o **pad**: preenche o resto até ficar 640x640\n",
    "\n",
    "Visualmente, fica assim:\n",
    "\n",
    "- conteúdo real ocupa 640x360\n",
    "- sobram “faixas” (geralmente em cima/baixo, ou esquerda/direita)\n",
    "- essas faixas recebem uma cor constante: `fill=(114,114,114)`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1722f001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagem processor\n",
    "@dataclass\n",
    "class YoloBatchItem:\n",
    "    img: torch.Tensor          # (3, H, W) float32 [0..1]\n",
    "    bboxes: torch.Tensor       # (N, 4) float32 (x,y,w,h) normalizado\n",
    "    cls: torch.Tensor          # (N, 1)\n",
    "    area: torch.Tensor         # (N,) float32 área em pixels (após resize/pad)\n",
    "    im_file: str\n",
    "    ori_shape: Tuple[int, int] # (h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f7db928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloImageProcessor:\n",
    "    \"\"\"\n",
    "    Processa imagem + bboxes YOLO e aplica augmentations.\n",
    "    Retorna tensores no padrão collate para o loss do YOLO.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, imgsz: int = 640, train: bool = True):\n",
    "        self.imgsz = imgsz\n",
    "        self.train = train\n",
    "\n",
    "        # mantém ratio com LongestMaxSize + PadIfNeeded\n",
    "        base = [\n",
    "            # redimensiona mantendo ratio\n",
    "            A.LongestMaxSize(max_size=imgsz, interpolation=cv2.INTER_LINEAR),\n",
    "            # pad para quadrado imgsz x imgsz (640x640, com borda cinza)\n",
    "            A.PadIfNeeded(\n",
    "                min_height=imgsz,\n",
    "                min_width=imgsz,\n",
    "                border_mode=cv2.BORDER_CONSTANT,\n",
    "                fill=(114, 114, 114),\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        if train:\n",
    "            aug = [\n",
    "                # geometria\n",
    "                # melhora a robustez contra pequenas mudanças de escala/rotação/perspectiva\n",
    "                A.Affine(\n",
    "                    shift_limit=0.05, # desloca até 5% da largura/altura\n",
    "                    scale_limit=0.20, # escala a imagem\n",
    "                    rotate_limit=15, # rotaciona até 15 graus\n",
    "                    border_mode=cv2.BORDER_CONSTANT, # adiciona borda\n",
    "                    fill=(114, 114, 114), # cor da borda\n",
    "                    p=0.9, # probabilidade\n",
    "                ),\n",
    "                # simula distorsão de perspectiva leve\n",
    "                A.Perspective(scale=(0.00, 0.05), p=0.15),\n",
    "\n",
    "                # cor/iluminação\n",
    "                A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=15, p=0.6), # muda matiz, saturação, valor\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.6), # simula variação de brilho/contraste\n",
    "                A.RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=0.2), # muda canais RGB\n",
    "\n",
    "                # blur leve\n",
    "                A.GaussianBlur(blur_limit=(3, 5), p=0.10),\n",
    "            ]\n",
    "        else:\n",
    "            aug = []\n",
    "\n",
    "        # faz com que as caixas(bboxes) sejam ajustadas às transformações\n",
    "        self.transform = A.Compose(\n",
    "            base + aug,\n",
    "            bbox_params=A.BboxParams(\n",
    "                format=\"yolo\",        # (x,y,w,h) normalizado\n",
    "                label_fields=[\"category_ids\"],\n",
    "                min_visibility=0.0,\n",
    "                clip=True,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def __call__(self, image_bgr: Any, bboxes_yolo: List[List[float]], cls_ids: List[int], im_file: str) -> YoloBatchItem:\n",
    "        \"\"\"\n",
    "        image_bgr: np.ndarray (H,W,3) BGR\n",
    "        bboxes_yolo: lista de [x,y,w,h] normalizados\n",
    "        cls_ids: lista de int\n",
    "        \"\"\"\n",
    "\n",
    "        # mantem shape original para retorno se necessário\n",
    "        h0, w0 = image_bgr.shape[:2]\n",
    "        image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # aplica transformações \n",
    "        out = self.transform(image=image_rgb, bboxes=bboxes_yolo, category_ids=cls_ids)\n",
    "        img = out[\"image\"]  # imagem transformada (H,W,3) RGB\n",
    "        bboxes = out[\"bboxes\"] # lista de [x,y,w,h] normalizados\n",
    "        cats = out[\"category_ids\"] # classes correspondentes\n",
    "\n",
    "        # converte a imagem para tensor, que é o formato esperado pelo modelo. HWC - CHW, float32 [0..1]\n",
    "        img_t = torch.from_numpy(img).permute(2, 0, 1).contiguous().float() / 255.0\n",
    "\n",
    "        # tratando os casos em que é passado apenas o fundo sem objetos, o que não gera bboxes\n",
    "        if len(bboxes) == 0:\n",
    "            bboxes_t = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            cls_t = torch.zeros((0, 1), dtype=torch.int64)\n",
    "            area_t = torch.zeros((0,), dtype=torch.float32)\n",
    "        else:\n",
    "            bboxes_t = torch.tensor(bboxes, dtype=torch.float32)         # (N,4) yolo norm\n",
    "            cls_t = torch.tensor(cats, dtype=torch.int64).view(-1, 1)  # (N,1)\n",
    "\n",
    "            # area_px = (w*imgsz) * (h*imgsz)\n",
    "            area_t = (bboxes_t[:, 2] * self.imgsz) * (bboxes_t[:, 3] * self.imgsz)\n",
    "\n",
    "        return YoloBatchItem(\n",
    "            img=img_t,\n",
    "            bboxes=bboxes_t,\n",
    "            cls=cls_t,\n",
    "            area=area_t,\n",
    "            im_file=im_file,\n",
    "            ori_shape=(h0, w0),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6adc2a3",
   "metadata": {},
   "source": [
    "## Leitura do dataset\n",
    "\n",
    "A ideia deste bloco é montar um **Dataset PyTorch** que:\n",
    "\n",
    "1. Lê o `data.yaml` no padrão YOLO\n",
    "2. Encontra todas as imagens do split\n",
    "3. Para cada imagem, acha o `.txt` correspondente em `labels/`\n",
    "4. Lê as bboxes no formato YOLO (`class x y w h`, tudo normalizado)\n",
    "5. Joga tudo no `YoloImageProcessor`\n",
    "6. Retorna um dicionário pronto para o `collate_fn` montar o batch do jeito que o **loss da Ultralytics** espera\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ef70e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_data_yaml(data_yaml: str) -> Dict[str, Any]:\n",
    "    with open(data_yaml, \"r\", encoding=\"utf-8\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "    \n",
    "def _resolve_path(base: str, p: str) -> str:\n",
    "    if os.path.isabs(p):\n",
    "        return p\n",
    "    return os.path.normpath(os.path.join(base, p))\n",
    "\n",
    "def _list_images_from_source(src: str) -> List[str]:\n",
    "    if os.path.isdir(src):\n",
    "        exts = (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.webp\")\n",
    "        files = []\n",
    "        for e in exts:\n",
    "            files.extend(glob.glob(os.path.join(src, e)))\n",
    "        return sorted(files)\n",
    "\n",
    "    if os.path.isfile(src) and src.lower().endswith(\".txt\"):\n",
    "        with open(src, \"r\", encoding=\"utf-8\") as f:\n",
    "            files = [ln.strip() for ln in f.readlines() if ln.strip()]\n",
    "        return files\n",
    "    \n",
    "def _img_to_label_path(im_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Convenção YOLO comum:\n",
    "      .../images/.../xxx.jpg -> .../labels/.../xxx.txt\n",
    "    \"\"\"\n",
    "    p = im_path.replace(os.sep + \"images\" + os.sep, os.sep + \"labels\" + os.sep)\n",
    "    p = os.path.splitext(p)[0] + \".txt\"\n",
    "    return p\n",
    "\n",
    "def _read_yolo_label_file(label_path: str) -> Tuple[List[int], List[List[float]]]:\n",
    "    \"\"\"\n",
    "    Retorna (cls_ids, bboxes_yolo_norm)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(label_path):\n",
    "        return [], []\n",
    "\n",
    "    cls_ids: List[int] = []\n",
    "    bboxes: List[List[float]] = []\n",
    "\n",
    "    with open(label_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) != 5:\n",
    "                continue\n",
    "            c = int(float(parts[0]))\n",
    "            x, y, w, h = map(float, parts[1:])\n",
    "            cls_ids.append(c)\n",
    "            bboxes.append([x, y, w, h])\n",
    "\n",
    "    return cls_ids, bboxes\n",
    "\n",
    "\n",
    "class YoloDetectionDataset(Dataset):\n",
    "    def __init__(self, data_yaml: str, split: str, imgsz: int = 640, train: bool = True):\n",
    "        \"\"\"\n",
    "        split: \"train\" ou \"val\"\n",
    "        train: define se usa augmentations ou não\n",
    "        \"\"\"\n",
    "        cfg = _read_data_yaml(data_yaml)\n",
    "        base = os.path.dirname(os.path.abspath(data_yaml))\n",
    "\n",
    "        # nomes no yaml podem ser list ou dict\n",
    "        names = cfg.get(\"names\", None)\n",
    "        if names is not None:\n",
    "            pass\n",
    "\n",
    "        src = cfg.get(split, None)\n",
    "        if src is None:\n",
    "            raise KeyError(f\"data.yaml não tem a chave '{split}'\")\n",
    "\n",
    "        src = _resolve_path(base, src)\n",
    "        self.image_paths = _list_images_from_source(src)\n",
    "\n",
    "        self.processor = YoloImageProcessor(imgsz=imgsz, train=train)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        im_path = self.image_paths[idx]\n",
    "        img_bgr = cv2.imread(im_path, cv2.IMREAD_COLOR)\n",
    "        if img_bgr is None:\n",
    "            raise FileNotFoundError(f\"Falha ao ler imagem: {im_path}\")\n",
    "\n",
    "        lab_path = _img_to_label_path(im_path)\n",
    "        cls_ids, bboxes = _read_yolo_label_file(lab_path)\n",
    "\n",
    "        item = self.processor(img_bgr, bboxes, cls_ids, im_file=im_path)\n",
    "\n",
    "        # o collate_fn vai montar o batch do jeito do YOLO loss.\n",
    "        return {\n",
    "            \"img\": item.img,\n",
    "            \"bboxes\": item.bboxes,\n",
    "            \"cls\": item.cls,\n",
    "            \"area\": item.area,\n",
    "            \"im_file\": item.im_file,\n",
    "            \"ori_shape\": item.ori_shape,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34252b30",
   "metadata": {},
   "source": [
    "## `collate_fn` - montando o batch do jeito que o YOLO loss gosta\n",
    "\n",
    "No PyTorch, o `Dataset.__getitem__()` devolve **um item por vez**.\n",
    "\n",
    "Mas o treino roda em **batch** (várias imagens juntas).  \n",
    "Quem transforma uma lista de itens em um batch é o `collate_fn`.\n",
    "\n",
    "`collate_fn` tem um objetivo bem específico:\n",
    "\n",
    "- **Empilhar as imagens** em um tensor `(B, 3, H, W)`\n",
    "- **Concatenar todas as bboxes do batch** em um único tensor `(N_total, 4)`\n",
    "- **Concatenar todas as classes** em `(N_total, 1)`\n",
    "- **Criar um vetor** `batch_idx` **de tamanho** `(N_total,)` **dizendo a qual imagem do batch cada bbox pertence**\n",
    "\n",
    "Essa última parte (`batch_idx`) é crucial, porque cada imagem tem um número variável de objetos, então você não consegue ter um tensor “fixo” como `(B, max_objs, 4)`.  \n",
    "A Ultralytics resolve isso com concatenação + índice.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d384df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    # empilha as imagens, cada imagem é um tensor (3,H,W) já normalizado [0..1]\n",
    "    imgs = torch.stack([e[\"img\"] for e in examples], dim=0)  # (B,3,H,W)\n",
    "\n",
    "    # os boxes não podem ser empilhados direto porque cada imagem pode ter número diferente de objetos nela\n",
    "    bboxes_list = []\n",
    "    cls_list = []\n",
    "    batch_idx_list = []\n",
    "\n",
    "    # para cada exemplo na batch\n",
    "    for i, e in enumerate(examples):\n",
    "        # número de boxes na imagem i\n",
    "        n = e[\"bboxes\"].shape[0]\n",
    "        # se não tem boxes, pula\n",
    "        if n == 0:\n",
    "            continue\n",
    "        # adiciona os boxes e classes\n",
    "        bboxes_list.append(e[\"bboxes\"])\n",
    "        cls_list.append(e[\"cls\"])\n",
    "        # índices da batch para cada box\n",
    "        # ex: se imagem i=2 tem n=7 então cria tensor [2,2,2,2,2,2,2] de shape (7,)\n",
    "        batch_idx_list.append(torch.full((n,), i, dtype=torch.int64))\n",
    "\n",
    "    # caso o batch não tenha boxes nenhum, cria tudo como zeros\n",
    "    # ex: todas imagens são fundo\n",
    "    if len(bboxes_list) == 0:\n",
    "        bboxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "        cls = torch.zeros((0, 1), dtype=torch.int64)\n",
    "        batch_idx = torch.zeros((0,), dtype=torch.int64)\n",
    "    else:\n",
    "        # concatena todos os boxes, classes e índices da batch\n",
    "        bboxes = torch.cat(bboxes_list, dim=0)\n",
    "        cls = torch.cat(cls_list, dim=0)\n",
    "        batch_idx = torch.cat(batch_idx_list, dim=0)\n",
    "\n",
    "    return {\n",
    "        \"img\": imgs,\n",
    "        \"bboxes\": bboxes,\n",
    "        \"cls\": cls,\n",
    "        \"batch_idx\": batch_idx,\n",
    "        # não usados no loss, mas util pra debug\n",
    "        \"im_file\": [e[\"im_file\"] for e in examples],\n",
    "        \"ori_shape\": [e[\"ori_shape\"] for e in examples],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b135c57",
   "metadata": {},
   "source": [
    "## `_first_tensor_sum`: “âncora” de gradiente quando o loss vira constante\n",
    "### Por que se precisa disso?\n",
    "Em detecção de objetos pode ocorrer de termos apenas o fundo (background) e nesses casos a loss acaba retornando um valor constante. Quando isso acontece o trainer vai tentar fazer o backward e pode dar um erro ou o treino fica incosistente. Ao pegar um valor escalar no momento de fazer a loss ele vai gerantir que o valor existirá - `loss = loss + 0.0 * anchor`\n",
    "\n",
    "## `ensure_yolo_criterion`: Cria/Ajusta a loss no device (CPU/GPU)\n",
    "Garante que o criterion exista, esteja setado e que todos os buffers estão no local correto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f779e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _first_tensor_sum(x):\n",
    "    \"\"\"Acha algum tensor em preds e retorna um escalar com grad para ancorar o grafo.\"\"\"\n",
    "    if torch.is_tensor(x):\n",
    "        return x.sum()\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        for t in x:\n",
    "            if torch.is_tensor(t):\n",
    "                return t.sum()\n",
    "            if isinstance(t, (list, tuple, dict)):\n",
    "                s = _first_tensor_sum(t)\n",
    "                if s is not None:\n",
    "                    return s\n",
    "    if isinstance(x, dict):\n",
    "        for v in x.values():\n",
    "            s = _first_tensor_sum(v)\n",
    "            if s is not None:\n",
    "                return s\n",
    "    return None\n",
    "\n",
    "\n",
    "def ensure_yolo_criterion(m):\n",
    "    defaults = {\"box\": 7.5, \"cls\": 0.5, \"dfl\": 1.5}\n",
    "\n",
    "    # prepara args como namespace que é o esperado pelo loss\n",
    "    # na prática garante que existam box, cls, dfl\n",
    "    args = getattr(m, \"args\", None)\n",
    "    # caso args seja dict ou None, cria namespace novo\n",
    "    if isinstance(args, dict):\n",
    "        m.args = SimpleNamespace(**{**defaults, **args})\n",
    "    elif args is None:\n",
    "        m.args = SimpleNamespace(**defaults)\n",
    "    # se já é namespace, só adiciona os defaults que faltam\n",
    "    else:\n",
    "        for k, v in defaults.items():\n",
    "            if not hasattr(m.args, k):\n",
    "                setattr(m.args, k, v)\n",
    "\n",
    "    # cria criterion\n",
    "    if getattr(m, \"criterion\", None) is None and hasattr(m, \"init_criterion\"):\n",
    "        try:\n",
    "            m.init_criterion()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if getattr(m, \"criterion\", None) is None:\n",
    "        m.criterion = v8DetectionLoss(m)\n",
    "\n",
    "    # garante que hyp esteja alinhado\n",
    "    crit = m.criterion\n",
    "    if hasattr(crit, \"hyp\"):\n",
    "        crit.hyp = m.args\n",
    "\n",
    "    # garante device consistente\n",
    "    device = next(m.parameters()).device\n",
    "\n",
    "    # 1) garante crit.device correto\n",
    "    if hasattr(crit, \"device\"):\n",
    "        crit.device = device\n",
    "\n",
    "    # 2) garante stride no mesmo device\n",
    "    if hasattr(crit, \"stride\"):\n",
    "        if torch.is_tensor(crit.stride):\n",
    "            crit.stride = crit.stride.to(device)\n",
    "\n",
    "    # 3) proj e qualquer tensor interno do loss para o device\n",
    "    if hasattr(crit, \"proj\") and torch.is_tensor(crit.proj):\n",
    "        crit.proj = crit.proj.to(device)\n",
    "\n",
    "    for name, val in vars(crit).items():\n",
    "        if torch.is_tensor(val) and val.device != device:\n",
    "            setattr(crit, name, val.to(device))\n",
    "\n",
    "    # 4) alguns modelos também têm m.stride como tensor fora do device\n",
    "    if hasattr(m, \"stride\") and torch.is_tensor(m.stride) and m.stride.device != device:\n",
    "        m.stride = m.stride.to(device)\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04341059",
   "metadata": {},
   "source": [
    "# `YoloHFTrainer.compute_loss` (YOLO + Hugging Face Trainer)\n",
    "\n",
    "A ideia central aqui é: **o `Trainer` do Hugging Face foi feito pensando em modelos “NLP-style”** (que recebem `model(**inputs)` e devolvem `loss` dentro de `outputs`).  \n",
    "No YOLO (Ultralytics), o fluxo real é outro:\n",
    "\n",
    "1. você passa **apenas a imagem** no forward: `preds = model(img)`\n",
    "2. você calcula o loss chamando um **criterion próprio**: `loss = criterion(preds, batch)`\n",
    "3. às vezes o batch não tem boxes e o loss pode virar **constante sem grad**\n",
    "\n",
    "Esse método adapta o Trainer para esse mundo.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a891790",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloHFTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        # manda para o device\n",
    "        raw_model = self.accelerator.unwrap_model(model) if hasattr(self, \"accelerator\") else model\n",
    "        device = next(raw_model.parameters()).device\n",
    "\n",
    "        # move inputs para o device do modelo\n",
    "        batch = {k: (v.to(device, non_blocking=True) if torch.is_tensor(v) else v) for k, v in inputs.items()}\n",
    "\n",
    "        # garante criterion\n",
    "        if getattr(raw_model, \"criterion\", None) is None or not callable(raw_model.criterion):\n",
    "            ensure_yolo_criterion(raw_model)\n",
    "\n",
    "        # forward + loss, garante gradiente pois aqui estamos treinando\n",
    "        with torch.enable_grad():\n",
    "            preds = raw_model(batch[\"img\"])                  # forward padrão\n",
    "            loss_out = raw_model.criterion(preds, batch)     # loss YOLO, loss_out = (loss, loss_items), loss_items = (box, cls, dfl)\n",
    "            \n",
    "        # guarda loss_items (box, cls, dfl) para logging por época, usado em TrainerCallback\n",
    "        self._last_loss_items = None\n",
    "        if isinstance(loss_out, (tuple, list)) and len(loss_out) >= 2 and torch.is_tensor(loss_out[1]):\n",
    "            li = loss_out[1].detach()\n",
    "            if li.numel() >= 3:\n",
    "                # (3,) -> box, cls, dfl\n",
    "                self._last_loss_items = li[:3].float().detach().cpu()\n",
    "\n",
    "        # extrai o loss total\n",
    "        if isinstance(loss_out, (tuple, list)):\n",
    "            loss = loss_out[0]\n",
    "        else:\n",
    "            loss = loss_out\n",
    "\n",
    "        # garante escalar pois o HF espera que seja escalar\n",
    "        if loss.numel() != 1:\n",
    "            loss = loss.mean()\n",
    "\n",
    "        # se batch não tem bbox, loss pode virar constante - sem grad\n",
    "        if not loss.requires_grad:\n",
    "            # tenta ancorar em algo que veio do forward\n",
    "            anchor = _first_tensor_sum(preds)\n",
    "\n",
    "            # se não der, ancora nos parâmetros treináveis (head-only)\n",
    "            if anchor is None or (torch.is_tensor(anchor) and not anchor.requires_grad):\n",
    "                trainable_sum = None\n",
    "                for p in raw_model.parameters():\n",
    "                    if p.requires_grad:\n",
    "                        trainable_sum = p.sum() if trainable_sum is None else (trainable_sum + p.sum())\n",
    "\n",
    "                if trainable_sum is None:\n",
    "                    raise RuntimeError(\"Nenhum parâmetro treinável. Você congelou tudo e não liberou o head corretamente.\")\n",
    "\n",
    "                anchor = trainable_sum\n",
    "\n",
    "            # linha que foi comentado em sima, não muda o valor mas força que ocorra o gradiente\n",
    "            loss = loss + 0.0 * anchor\n",
    "\n",
    "\n",
    "        return (loss, loss_out) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8868dd71",
   "metadata": {},
   "source": [
    "# `build_yolo_model` + `freeze_yolo_head_only` - carregando o YOLO e treinando só o head\n",
    "\n",
    "1. **`build_yolo_model`**: carrega um `.pt` do Ultralytics e devolve o `BaseModel` pronto pra usar no pipeline (criterion alinhado, train mode ligado).\n",
    "2. **`freeze_yolo_head_only`**: congela o modelo inteiro e libera **somente o head de detecção**, para fine-tuning leve (Aqui é onde está o segredo).\n",
    "\n",
    "Funciona porque:\n",
    "- o backbone já “enxerga” padrões visuais genéricos\n",
    "- o que você mais precisa adaptar são os **últimos blocos** que mapeiam features - caixas/classes (fruta/bomba)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d659965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_yolo_model(weights_pt: str):\n",
    "    y = YOLO(weights_pt)\n",
    "    m = y.model  # Ultralytics BaseModel\n",
    "\n",
    "    # garante args + criterion + hyp\n",
    "    m = ensure_yolo_criterion(m)\n",
    "\n",
    "    # coloca o modelo em modo treino\n",
    "    m.train()\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581a4b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_yolo_head_only(m):\n",
    "    # congela tudo\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad_(False)\n",
    "\n",
    "    # acha o detect head de forma robusta\n",
    "    # porque no Ultralytics o último módlo pode conter: Detect, v8Detect, DetectMultiBackend, etc\n",
    "    # procuramos apenas por \"detect\"\n",
    "    head = None\n",
    "    if hasattr(m, \"model\"):  # Ultralytics BaseModel: m.model é uma lista de camadas\n",
    "        for layer in reversed(m.model):\n",
    "            name = layer.__class__.__name__.lower()\n",
    "            if \"detect\" in name:  # Detect, v8Detect, etc.\n",
    "                head = layer\n",
    "                break\n",
    "        if head is None:\n",
    "            head = m.model[-1]  # fallback\n",
    "    else:\n",
    "        head = m  # fallback extremo\n",
    "\n",
    "    # libera só o head que são os casos que queremos\n",
    "    for p in head.parameters():\n",
    "        p.requires_grad_(True)\n",
    "\n",
    "    return head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb806947",
   "metadata": {},
   "source": [
    "# Painel para o treinamento\n",
    "Aqui são funções que obtém as mesmas informações existentes no treinamento da Ultralytics, não irei entrar em detalhes da implementação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0fd6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_ultra_metrics(val_out):\n",
    "    \"\"\"\n",
    "    Tenta extrair P, R, mAP50, mAP50-95 do retorno do YOLO.val().\n",
    "    \"\"\"\n",
    "    P = R = mAP50 = mAP5095 = float(\"nan\")\n",
    "\n",
    "    # caminho mais comum: metrics.box.mp / mr / map50 / map\n",
    "    if hasattr(val_out, \"box\"):\n",
    "        box = val_out.box\n",
    "        if hasattr(box, \"mp\"):    P = float(box.mp)\n",
    "        if hasattr(box, \"mr\"):    R = float(box.mr)\n",
    "        if hasattr(box, \"map50\"): mAP50 = float(box.map50)\n",
    "        if hasattr(box, \"map\"):   mAP5095 = float(box.map)\n",
    "\n",
    "    # fallback: results_dict\n",
    "    if (math.isnan(P) or math.isnan(R) or math.isnan(mAP50) or math.isnan(mAP5095)) and hasattr(val_out, \"results_dict\"):\n",
    "        rd = val_out.results_dict\n",
    "        # tenta achar pelas chaves mais frequentes\n",
    "        for k, v in rd.items():\n",
    "            lk = str(k).lower()\n",
    "            if \"precision\" in lk and \"box\" in lk:\n",
    "                P = float(v)\n",
    "            elif \"recall\" in lk and \"box\" in lk:\n",
    "                R = float(v)\n",
    "            elif \"map50\" in lk and \"box\" in lk:\n",
    "                mAP50 = float(v)\n",
    "            elif (\"map\" in lk and \"box\" in lk) and (\"map50\" not in lk):\n",
    "                mAP5095 = float(v)\n",
    "\n",
    "    return P, R, mAP50, mAP5095\n",
    "\n",
    "\n",
    "class YoloPrettyPrintCallback(TrainerCallback):\n",
    "    def __init__(self, trainer, data_yaml, imgsz, device=0, val_batch=16, weights_stub=\"./yolo11n.pt\", run_val_every_epochs=1):\n",
    "        self.trainer = trainer\n",
    "        self.data_yaml = data_yaml\n",
    "        self.imgsz = imgsz\n",
    "        self.device = device\n",
    "        self.val_batch = val_batch\n",
    "        self.weights_stub = weights_stub\n",
    "        self.run_val_every_epochs = run_val_every_epochs\n",
    "\n",
    "        self.epoch_i = 0\n",
    "        self.reset_epoch_acc()\n",
    "\n",
    "    def reset_epoch_acc(self):\n",
    "        self.n_steps = 0\n",
    "        self.sum_box = 0.0\n",
    "        self.sum_cls = 0.0\n",
    "        self.sum_dfl = 0.0\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        # pega o último loss_items salvo no compute_loss\n",
    "        li = getattr(self.trainer, \"_last_loss_items\", None)\n",
    "        if li is None:\n",
    "            return\n",
    "\n",
    "        self.sum_box += float(li[0])\n",
    "        self.sum_cls += float(li[1])\n",
    "        self.sum_dfl += float(li[2])\n",
    "        self.n_steps += 1\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        self.epoch_i += 1\n",
    "        total_epochs = int(args.num_train_epochs)\n",
    "\n",
    "        # médias da época (loss)\n",
    "        denom = max(1, self.n_steps)\n",
    "        box_loss = self.sum_box / denom\n",
    "        cls_loss = self.sum_cls / denom\n",
    "        dfl_loss = self.sum_dfl / denom\n",
    "\n",
    "        # métricas de detecção (P/R/mAP)\n",
    "        P = R = mAP50 = mAP5095 = float(\"nan\")\n",
    "\n",
    "        do_val = (self.run_val_every_epochs > 0) and (self.epoch_i % self.run_val_every_epochs == 0)\n",
    "        if do_val:\n",
    "            raw_model = self.trainer.accelerator.unwrap_model(self.trainer.model) if hasattr(self.trainer, \"accelerator\") else self.trainer.model\n",
    "\n",
    "            y = YOLO(self.weights_stub)  # só para montar pipeline de val\n",
    "            y.model.load_state_dict(copy.deepcopy(raw_model.state_dict()), strict=False)\n",
    "            val_out = y.val(\n",
    "                data=self.data_yaml,\n",
    "                imgsz=self.imgsz,\n",
    "                batch=self.val_batch,\n",
    "                device=self.device,\n",
    "                plots=False,\n",
    "                save=False,\n",
    "                verbose=False,\n",
    "            )\n",
    "            P, R, mAP50, mAP5095 = _extract_ultra_metrics(val_out)\n",
    "\n",
    "        print(f\"\\nEpoch  {self.epoch_i}/{total_epochs}\")\n",
    "        print(f\"box_loss  {box_loss:.4f}\")\n",
    "        print(f\"cls_loss  {cls_loss:.4f}\")\n",
    "        print(f\"dfl_loss  {dfl_loss:.4f}\")\n",
    "        print(f\"Box(P  {P:.3f}\")\n",
    "        print(f\"R  {R:.3f}\")\n",
    "        print(f\"mAP50  {mAP50:.2f}\")\n",
    "        print(f\"mAP50-95)  {mAP5095:.2f}\")\n",
    "\n",
    "        self.reset_epoch_acc()\n",
    "        return control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43589bfb",
   "metadata": {},
   "source": [
    "#  `main()` O pipeline do finetunning\n",
    "\n",
    "- **Dataset YOLO (imagens + labels)** → `YoloDetectionDataset`\n",
    "- **Modelo Ultralytics (YOLO)** → `build_yolo_model`\n",
    "- **Treino controlado pelo Hugging Face Trainer** → `YoloHFTrainer`\n",
    "- **Validação no estilo Ultralytics** → `YoloPrettyPrintCallback`\n",
    "\n",
    "A lógica geral é:\n",
    "\n",
    "1. Define caminhos e parâmetros (data.yaml, imgsz, pesos base).\n",
    "2. Carrega dataset de treino e validação.\n",
    "3. Carrega modelo YOLO, manda pra GPU e garante o criterion no device certo.\n",
    "4. Congela tudo e libera só o head (fine-tuning leve).\n",
    "5. Monta `TrainingArguments`.\n",
    "6. Cria otimizador **só com parâmetros treináveis**.\n",
    "7. Instancia o `Trainer` customizado.\n",
    "8. Adiciona callback para imprimir loss + métricas por época.\n",
    "9. Treina.\n",
    "10. Salva o `state_dict` e limpa GPU.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ded682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 1. caminhos e parâmetros\n",
    "    data_yaml = \"../datasets/fruitninja_yolo/data.yaml\"\n",
    "    imgsz = 640\n",
    "\n",
    "    weights_pt = \"./yolo11n.pt\" # pesos base\n",
    "\n",
    "    # 2. carrega os df de treino e validação\n",
    "    train_ds = YoloDetectionDataset(data_yaml=data_yaml, split=\"train\", imgsz=imgsz, train=True)\n",
    "    val_ds   = YoloDetectionDataset(data_yaml=data_yaml, split=\"val\", imgsz=imgsz, train=False)\n",
    "\n",
    "    # 3. Carrega modelo YOLO, manda pra GPU e garante o criterion no device certo\n",
    "    model = build_yolo_model(weights_pt).cuda()\n",
    "    ensure_yolo_criterion(model)\n",
    "    \n",
    "    # 4. Congela tudo e libera só o head (fine-tuning leve).\n",
    "    head = freeze_yolo_head_only(model)\n",
    "    \n",
    "    # apenas para visualização\n",
    "    trainable_named = [(n, p) for n, p in model.named_parameters() if p.requires_grad]\n",
    "    print(\"Trainable params:\", len(trainable_named))\n",
    "    print(\"Exemplo:\", [(n, p.shape) for n, p in trainable_named[:10]])\n",
    "\n",
    "    # 5. Monta `TrainingArguments`.\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"../models/runs/fruitninja_yolo_manual/\",\n",
    "        per_device_train_batch_size=16, # \"amostra\" com 16 imagens\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=120, # quantas execuções fazer por batch, garante estabilização\n",
    "\n",
    "        logging_steps=50, # fazer um log a cada 50 passos\n",
    "        save_steps=200, # fazer um checkpoint a cada 200 passos\n",
    "        save_total_limit=2, # só mantém 2 checkpoints\n",
    "        eval_strategy=\"no\", # não faz evaluate pois foi montado o YoloPrettyPrintCallback\n",
    "\n",
    "        fp16=True, # ativa mixed precision, o que aumenta perfomance e consome menos VRAM\n",
    "        dataloader_num_workers=0, # evita bugs no jupyter, se mudar aqui quebra, mas em código normal tende a funcionar\n",
    "\n",
    "        remove_unused_columns=False, # como o batch é personalizado (bboxes, cls, batch_idx) o trainer por padrão pode tentar limpar essas colunas, aqui estamos impedindo isso de acontecer\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    # 6. Cria otimizador só com parâmetros treináveis.\n",
    "    trainable_params = [p for _, p in trainable_named]\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        trainable_params,\n",
    "        lr=2e-4,\n",
    "        weight_decay=1e-4,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8,\n",
    "    )\n",
    "\n",
    "    # 7. Instancia o `Trainer` customizado.\n",
    "    trainer = YoloHFTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        data_collator=collate_fn,\n",
    "        tokenizer=None,  # aqui não precisa pois já foi passado dentro do Dataset\n",
    "        optimizers=(optimizer, None)\n",
    "    )\n",
    "    \n",
    "    print(\"model device:\", next(model.parameters()).device)\n",
    "    print(\"crit.device:\", model.criterion.device)\n",
    "    print(\"crit.stride:\", type(model.criterion.stride), getattr(model.criterion.stride, \"device\", \"no-device\"))\n",
    "    print(\"crit.proj:\", model.criterion.proj.device)\n",
    "\n",
    "    # 8. Adiciona callback para imprimir loss + métricas por época.\n",
    "    trainer.add_callback(\n",
    "        YoloPrettyPrintCallback(\n",
    "            trainer=trainer,\n",
    "            data_yaml=data_yaml,\n",
    "            imgsz=imgsz,\n",
    "            device=0,\n",
    "            val_batch=16,\n",
    "            weights_stub=weights_pt,        \n",
    "            run_val_every_epochs=1,         # valida toda epoch\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 9. Treina\n",
    "    trainer.train()\n",
    "\n",
    "    # 10. Salva o `state_dict` e limpa GPU.\n",
    "    os.makedirs(training_args.output_dir, exist_ok=True)\n",
    "    out_path = os.path.join(training_args.output_dir, \"yolo_state_dict.pt\")\n",
    "    torch.save(model.state_dict(), out_path)\n",
    "    print(\"Salvo em:\", out_path)\n",
    "\n",
    "    del model\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b80decc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pichau\\AppData\\Local\\Temp\\ipykernel_29488\\2388210312.py:28: UserWarning: Argument(s) 'shift_limit, scale_limit, rotate_limit' are not valid for transform Affine\n",
      "  A.Affine(\n",
      "C:\\Users\\Pichau\\AppData\\Local\\Temp\\ipykernel_29488\\2554395195.py:55: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `YoloHFTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = YoloHFTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 67\n",
      "Exemplo: [('model.23.cv2.0.0.conv.weight', torch.Size([64, 64, 3, 3])), ('model.23.cv2.0.0.bn.weight', torch.Size([64])), ('model.23.cv2.0.0.bn.bias', torch.Size([64])), ('model.23.cv2.0.1.conv.weight', torch.Size([64, 64, 3, 3])), ('model.23.cv2.0.1.bn.weight', torch.Size([64])), ('model.23.cv2.0.1.bn.bias', torch.Size([64])), ('model.23.cv2.0.2.weight', torch.Size([64, 64, 1, 1])), ('model.23.cv2.0.2.bias', torch.Size([64])), ('model.23.cv2.1.0.conv.weight', torch.Size([64, 128, 3, 3])), ('model.23.cv2.1.0.bn.weight', torch.Size([64]))]\n",
      "criterion: <class 'ultralytics.utils.loss.v8DetectionLoss'> callable: True\n",
      "hyp: <class 'types.SimpleNamespace'> box: 7.5\n",
      "model device: cuda:0\n",
      "crit.device: cuda:0\n",
      "crit.stride: <class 'torch.Tensor'> cuda:0\n",
      "crit.proj: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1320' max='1320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1320/1320 11:56, Epoch 120/120]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>24.147600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>15.392900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>14.040900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>12.852600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>12.393100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>11.709000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>11.297000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>10.960500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>10.625900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>10.384700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>10.153900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>9.954900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>9.688300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>9.647600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>9.418000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>9.363100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>9.259600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>9.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>9.083000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>8.934800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>8.629800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>8.671500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>8.488900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>8.522100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>8.839100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>8.342000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 200.131.0 MB/s, size: 21.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 55.4Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.4s0.2ss\n",
      "                   all         61        112      0.779      0.198      0.303      0.196\n",
      "Speed: 0.6ms preprocess, 5.5ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "\n",
      "Epoch  1/120\n",
      "box_loss  1.3781\n",
      "cls_loss  5.3145\n",
      "dfl_loss  1.2965\n",
      "Box(P  0.779\n",
      "R  0.198\n",
      "mAP50  0.30\n",
      "mAP50-95)  0.20\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 311.268.9 MB/s, size: 24.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.8Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.415      0.478      0.544      0.356\n",
      "Speed: 0.7ms preprocess, 4.4ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "\n",
      "Epoch  2/120\n",
      "box_loss  1.1864\n",
      "cls_loss  2.0321\n",
      "dfl_loss  1.1833\n",
      "Box(P  0.415\n",
      "R  0.478\n",
      "mAP50  0.54\n",
      "mAP50-95)  0.36\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 299.233.8 MB/s, size: 23.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.656      0.713      0.622      0.385\n",
      "Speed: 0.7ms preprocess, 4.2ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "\n",
      "Epoch  3/120\n",
      "box_loss  1.0800\n",
      "cls_loss  1.3085\n",
      "dfl_loss  1.1331\n",
      "Box(P  0.656\n",
      "R  0.713\n",
      "mAP50  0.62\n",
      "mAP50-95)  0.38\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 222.115.7 MB/s, size: 20.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.3ss\n",
      "                   all         61        112      0.666      0.767      0.727      0.464\n",
      "Speed: 0.7ms preprocess, 4.4ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "\n",
      "Epoch  4/120\n",
      "box_loss  1.0596\n",
      "cls_loss  1.1563\n",
      "dfl_loss  1.1288\n",
      "Box(P  0.666\n",
      "R  0.767\n",
      "mAP50  0.73\n",
      "mAP50-95)  0.46\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 301.846.6 MB/s, size: 23.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.7Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.5it/s 2.8s0.3ss\n",
      "                   all         61        112      0.699      0.678      0.798      0.517\n",
      "Speed: 0.8ms preprocess, 4.6ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "\n",
      "Epoch  5/120\n",
      "box_loss  0.9761\n",
      "cls_loss  1.0430\n",
      "dfl_loss  1.0880\n",
      "Box(P  0.699\n",
      "R  0.678\n",
      "mAP50  0.80\n",
      "mAP50-95)  0.52\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 279.328.9 MB/s, size: 25.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.8Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.4it/s 2.8s0.3ss\n",
      "                   all         61        112      0.761      0.793      0.857      0.571\n",
      "Speed: 0.6ms preprocess, 4.5ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "\n",
      "Epoch  6/120\n",
      "box_loss  0.9548\n",
      "cls_loss  0.9724\n",
      "dfl_loss  1.0848\n",
      "Box(P  0.761\n",
      "R  0.793\n",
      "mAP50  0.86\n",
      "mAP50-95)  0.57\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 250.640.0 MB/s, size: 24.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 40.5Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.4it/s 2.8s0.3ss\n",
      "                   all         61        112      0.823      0.637      0.877      0.581\n",
      "Speed: 0.7ms preprocess, 4.9ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "\n",
      "Epoch  7/120\n",
      "box_loss  0.9406\n",
      "cls_loss  0.9667\n",
      "dfl_loss  1.0771\n",
      "Box(P  0.823\n",
      "R  0.637\n",
      "mAP50  0.88\n",
      "mAP50-95)  0.58\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 260.349.6 MB/s, size: 23.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.9Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.5it/s 2.7s1.0ss\n",
      "                   all         61        112      0.779      0.794      0.886      0.599\n",
      "Speed: 0.7ms preprocess, 4.1ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "\n",
      "Epoch  8/120\n",
      "box_loss  0.9272\n",
      "cls_loss  0.9365\n",
      "dfl_loss  1.0576\n",
      "Box(P  0.779\n",
      "R  0.794\n",
      "mAP50  0.89\n",
      "mAP50-95)  0.60\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 253.525.8 MB/s, size: 23.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.5it/s 2.6s0.3ss\n",
      "                   all         61        112      0.763      0.826      0.872      0.589\n",
      "Speed: 0.8ms preprocess, 4.3ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "\n",
      "Epoch  9/120\n",
      "box_loss  0.9259\n",
      "cls_loss  0.8935\n",
      "dfl_loss  1.0804\n",
      "Box(P  0.763\n",
      "R  0.826\n",
      "mAP50  0.87\n",
      "mAP50-95)  0.59\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 263.057.5 MB/s, size: 24.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.5it/s 2.6s0.2ss\n",
      "                   all         61        112      0.716      0.789      0.871      0.605\n",
      "Speed: 0.6ms preprocess, 4.7ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "\n",
      "Epoch  10/120\n",
      "box_loss  0.8607\n",
      "cls_loss  0.8632\n",
      "dfl_loss  1.0302\n",
      "Box(P  0.716\n",
      "R  0.789\n",
      "mAP50  0.87\n",
      "mAP50-95)  0.61\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 242.539.6 MB/s, size: 21.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.4s0.2ss\n",
      "                   all         61        112      0.737      0.744      0.867      0.601\n",
      "Speed: 0.7ms preprocess, 4.1ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "\n",
      "Epoch  11/120\n",
      "box_loss  0.8435\n",
      "cls_loss  0.8619\n",
      "dfl_loss  1.0221\n",
      "Box(P  0.737\n",
      "R  0.744\n",
      "mAP50  0.87\n",
      "mAP50-95)  0.60\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 351.0100.1 MB/s, size: 27.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.4s0.3ss\n",
      "                   all         61        112       0.72      0.701      0.861      0.612\n",
      "Speed: 0.7ms preprocess, 4.9ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "\n",
      "Epoch  12/120\n",
      "box_loss  0.8089\n",
      "cls_loss  0.8176\n",
      "dfl_loss  1.0110\n",
      "Box(P  0.720\n",
      "R  0.701\n",
      "mAP50  0.86\n",
      "mAP50-95)  0.61\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 279.363.7 MB/s, size: 25.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112       0.74      0.828      0.893      0.623\n",
      "Speed: 0.6ms preprocess, 4.0ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "\n",
      "Epoch  13/120\n",
      "box_loss  0.8477\n",
      "cls_loss  0.8256\n",
      "dfl_loss  1.0212\n",
      "Box(P  0.740\n",
      "R  0.828\n",
      "mAP50  0.89\n",
      "mAP50-95)  0.62\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 333.262.3 MB/s, size: 28.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.5s0.2ss\n",
      "                   all         61        112      0.728      0.842      0.904      0.647\n",
      "Speed: 0.9ms preprocess, 4.9ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "\n",
      "Epoch  14/120\n",
      "box_loss  0.7922\n",
      "cls_loss  0.7885\n",
      "dfl_loss  1.0047\n",
      "Box(P  0.728\n",
      "R  0.842\n",
      "mAP50  0.90\n",
      "mAP50-95)  0.65\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 315.344.7 MB/s, size: 24.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.4s0.2ss\n",
      "                   all         61        112       0.77      0.866       0.91      0.662\n",
      "Speed: 0.6ms preprocess, 5.1ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "\n",
      "Epoch  15/120\n",
      "box_loss  0.7487\n",
      "cls_loss  0.7732\n",
      "dfl_loss  0.9815\n",
      "Box(P  0.770\n",
      "R  0.866\n",
      "mAP50  0.91\n",
      "mAP50-95)  0.66\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 250.336.5 MB/s, size: 22.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.5s0.2ss\n",
      "                   all         61        112      0.789      0.879      0.902      0.655\n",
      "Speed: 0.7ms preprocess, 4.9ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "\n",
      "Epoch  16/120\n",
      "box_loss  0.7469\n",
      "cls_loss  0.7768\n",
      "dfl_loss  0.9801\n",
      "Box(P  0.789\n",
      "R  0.879\n",
      "mAP50  0.90\n",
      "mAP50-95)  0.65\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 221.911.6 MB/s, size: 20.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.5s0.2ss\n",
      "                   all         61        112      0.713      0.882      0.907      0.654\n",
      "Speed: 0.6ms preprocess, 4.7ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "\n",
      "Epoch  17/120\n",
      "box_loss  0.7254\n",
      "cls_loss  0.7704\n",
      "dfl_loss  0.9665\n",
      "Box(P  0.713\n",
      "R  0.882\n",
      "mAP50  0.91\n",
      "mAP50-95)  0.65\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 338.383.0 MB/s, size: 25.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.5s0.3ss\n",
      "                   all         61        112      0.761      0.848      0.917      0.657\n",
      "Speed: 0.8ms preprocess, 4.3ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "\n",
      "Epoch  18/120\n",
      "box_loss  0.6958\n",
      "cls_loss  0.7578\n",
      "dfl_loss  0.9516\n",
      "Box(P  0.761\n",
      "R  0.848\n",
      "mAP50  0.92\n",
      "mAP50-95)  0.66\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 265.438.0 MB/s, size: 24.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.9Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.5it/s 2.7s0.3ss\n",
      "                   all         61        112      0.768      0.816      0.913      0.668\n",
      "Speed: 0.8ms preprocess, 5.2ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "\n",
      "Epoch  19/120\n",
      "box_loss  0.7451\n",
      "cls_loss  0.7599\n",
      "dfl_loss  0.9769\n",
      "Box(P  0.768\n",
      "R  0.816\n",
      "mAP50  0.91\n",
      "mAP50-95)  0.67\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 349.758.0 MB/s, size: 27.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.5s0.2ss\n",
      "                   all         61        112      0.819      0.861      0.921      0.662\n",
      "Speed: 0.7ms preprocess, 4.5ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "\n",
      "Epoch  20/120\n",
      "box_loss  0.7020\n",
      "cls_loss  0.7208\n",
      "dfl_loss  0.9594\n",
      "Box(P  0.819\n",
      "R  0.861\n",
      "mAP50  0.92\n",
      "mAP50-95)  0.66\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 328.161.0 MB/s, size: 26.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.4s0.2ss\n",
      "                   all         61        112       0.89      0.804      0.917      0.673\n",
      "Speed: 0.7ms preprocess, 4.1ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "\n",
      "Epoch  21/120\n",
      "box_loss  0.6833\n",
      "cls_loss  0.7182\n",
      "dfl_loss  0.9623\n",
      "Box(P  0.890\n",
      "R  0.804\n",
      "mAP50  0.92\n",
      "mAP50-95)  0.67\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 350.583.8 MB/s, size: 27.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.1Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.3s0.2ss\n",
      "                   all         61        112      0.805      0.866      0.932      0.694\n",
      "Speed: 0.7ms preprocess, 4.3ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "\n",
      "Epoch  22/120\n",
      "box_loss  0.6829\n",
      "cls_loss  0.7003\n",
      "dfl_loss  0.9586\n",
      "Box(P  0.805\n",
      "R  0.866\n",
      "mAP50  0.93\n",
      "mAP50-95)  0.69\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 316.055.2 MB/s, size: 28.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.1Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.3s0.2ss\n",
      "                   all         61        112      0.834      0.831      0.923      0.665\n",
      "Speed: 0.6ms preprocess, 4.4ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "\n",
      "Epoch  23/120\n",
      "box_loss  0.6560\n",
      "cls_loss  0.7083\n",
      "dfl_loss  0.9390\n",
      "Box(P  0.834\n",
      "R  0.831\n",
      "mAP50  0.92\n",
      "mAP50-95)  0.66\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 268.956.3 MB/s, size: 26.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.9Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.3s0.2ss\n",
      "                   all         61        112      0.949      0.842      0.926      0.676\n",
      "Speed: 0.7ms preprocess, 4.4ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "\n",
      "Epoch  24/120\n",
      "box_loss  0.6186\n",
      "cls_loss  0.6634\n",
      "dfl_loss  0.9367\n",
      "Box(P  0.949\n",
      "R  0.842\n",
      "mAP50  0.93\n",
      "mAP50-95)  0.68\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 224.923.3 MB/s, size: 22.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 58.6Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.5s0.2ss\n",
      "                   all         61        112      0.954      0.781      0.934      0.704\n",
      "Speed: 0.6ms preprocess, 4.3ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "\n",
      "Epoch  25/120\n",
      "box_loss  0.6523\n",
      "cls_loss  0.7117\n",
      "dfl_loss  0.9400\n",
      "Box(P  0.954\n",
      "R  0.781\n",
      "mAP50  0.93\n",
      "mAP50-95)  0.70\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 321.949.8 MB/s, size: 24.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.845      0.855      0.934      0.702\n",
      "Speed: 0.7ms preprocess, 4.2ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "\n",
      "Epoch  26/120\n",
      "box_loss  0.6113\n",
      "cls_loss  0.6979\n",
      "dfl_loss  0.9256\n",
      "Box(P  0.845\n",
      "R  0.855\n",
      "mAP50  0.93\n",
      "mAP50-95)  0.70\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 283.632.0 MB/s, size: 26.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.3s0.2ss\n",
      "                   all         61        112      0.826      0.866      0.929      0.703\n",
      "Speed: 0.7ms preprocess, 4.1ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "\n",
      "Epoch  27/120\n",
      "box_loss  0.6174\n",
      "cls_loss  0.6978\n",
      "dfl_loss  0.9194\n",
      "Box(P  0.826\n",
      "R  0.866\n",
      "mAP50  0.93\n",
      "mAP50-95)  0.70\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 279.241.4 MB/s, size: 24.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.7Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.5s0.3ss\n",
      "                   all         61        112      0.925      0.836      0.936      0.704\n",
      "Speed: 0.7ms preprocess, 4.2ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "\n",
      "Epoch  28/120\n",
      "box_loss  0.6610\n",
      "cls_loss  0.6759\n",
      "dfl_loss  0.9531\n",
      "Box(P  0.925\n",
      "R  0.836\n",
      "mAP50  0.94\n",
      "mAP50-95)  0.70\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 278.345.8 MB/s, size: 27.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.1Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.5s0.3ss\n",
      "                   all         61        112      0.915      0.832      0.933      0.715\n",
      "Speed: 0.5ms preprocess, 3.9ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "\n",
      "Epoch  29/120\n",
      "box_loss  0.6036\n",
      "cls_loss  0.6489\n",
      "dfl_loss  0.9157\n",
      "Box(P  0.915\n",
      "R  0.832\n",
      "mAP50  0.93\n",
      "mAP50-95)  0.71\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 237.018.3 MB/s, size: 22.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.4it/s 2.8s0.3ss\n",
      "                   all         61        112      0.892      0.847      0.938      0.715\n",
      "Speed: 0.7ms preprocess, 5.0ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "\n",
      "Epoch  30/120\n",
      "box_loss  0.5886\n",
      "cls_loss  0.6412\n",
      "dfl_loss  0.9182\n",
      "Box(P  0.892\n",
      "R  0.847\n",
      "mAP50  0.94\n",
      "mAP50-95)  0.72\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 303.539.7 MB/s, size: 23.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.1Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.3s0.2ss\n",
      "                   all         61        112      0.882      0.835      0.925      0.715\n",
      "Speed: 0.7ms preprocess, 4.0ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "\n",
      "Epoch  31/120\n",
      "box_loss  0.5698\n",
      "cls_loss  0.6377\n",
      "dfl_loss  0.9236\n",
      "Box(P  0.882\n",
      "R  0.835\n",
      "mAP50  0.93\n",
      "mAP50-95)  0.72\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 287.820.7 MB/s, size: 26.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.8Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.923      0.855      0.939      0.723\n",
      "Speed: 0.7ms preprocess, 4.4ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "\n",
      "Epoch  32/120\n",
      "box_loss  0.5885\n",
      "cls_loss  0.6325\n",
      "dfl_loss  0.9262\n",
      "Box(P  0.923\n",
      "R  0.855\n",
      "mAP50  0.94\n",
      "mAP50-95)  0.72\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 230.524.0 MB/s, size: 20.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.9Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.914      0.841      0.937      0.722\n",
      "Speed: 0.6ms preprocess, 4.1ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "\n",
      "Epoch  33/120\n",
      "box_loss  0.5792\n",
      "cls_loss  0.6308\n",
      "dfl_loss  0.9202\n",
      "Box(P  0.914\n",
      "R  0.841\n",
      "mAP50  0.94\n",
      "mAP50-95)  0.72\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 271.916.0 MB/s, size: 20.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.7Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.947      0.837      0.936       0.73\n",
      "Speed: 0.7ms preprocess, 4.5ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "\n",
      "Epoch  34/120\n",
      "box_loss  0.5943\n",
      "cls_loss  0.6121\n",
      "dfl_loss  0.9179\n",
      "Box(P  0.947\n",
      "R  0.837\n",
      "mAP50  0.94\n",
      "mAP50-95)  0.73\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 351.759.1 MB/s, size: 28.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.9ss\n",
      "                   all         61        112      0.877      0.858      0.941      0.714\n",
      "Speed: 0.7ms preprocess, 3.9ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "\n",
      "Epoch  35/120\n",
      "box_loss  0.5620\n",
      "cls_loss  0.6179\n",
      "dfl_loss  0.9056\n",
      "Box(P  0.877\n",
      "R  0.858\n",
      "mAP50  0.94\n",
      "mAP50-95)  0.71\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 311.066.4 MB/s, size: 24.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.4s0.2ss\n",
      "                   all         61        112      0.956      0.819      0.944      0.733\n",
      "Speed: 0.6ms preprocess, 4.5ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "\n",
      "Epoch  36/120\n",
      "box_loss  0.5662\n",
      "cls_loss  0.6289\n",
      "dfl_loss  0.9151\n",
      "Box(P  0.956\n",
      "R  0.819\n",
      "mAP50  0.94\n",
      "mAP50-95)  0.73\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 287.259.7 MB/s, size: 22.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.3ss\n",
      "                   all         61        112      0.947      0.844      0.949      0.734\n",
      "Speed: 0.8ms preprocess, 4.0ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "\n",
      "Epoch  37/120\n",
      "box_loss  0.5512\n",
      "cls_loss  0.6128\n",
      "dfl_loss  0.9019\n",
      "Box(P  0.947\n",
      "R  0.844\n",
      "mAP50  0.95\n",
      "mAP50-95)  0.73\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 291.167.4 MB/s, size: 28.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112       0.86      0.866      0.945      0.745\n",
      "Speed: 0.7ms preprocess, 4.2ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "\n",
      "Epoch  38/120\n",
      "box_loss  0.5486\n",
      "cls_loss  0.6075\n",
      "dfl_loss  0.9022\n",
      "Box(P  0.860\n",
      "R  0.866\n",
      "mAP50  0.95\n",
      "mAP50-95)  0.75\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 255.742.0 MB/s, size: 24.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.8Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.3s0.2ss\n",
      "                   all         61        112      0.907      0.874       0.95      0.754\n",
      "Speed: 0.7ms preprocess, 3.8ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "\n",
      "Epoch  39/120\n",
      "box_loss  0.5222\n",
      "cls_loss  0.5816\n",
      "dfl_loss  0.8923\n",
      "Box(P  0.907\n",
      "R  0.874\n",
      "mAP50  0.95\n",
      "mAP50-95)  0.75\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 292.043.2 MB/s, size: 22.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.9Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.3s0.2ss\n",
      "                   all         61        112       0.94      0.817      0.944      0.734\n",
      "Speed: 0.7ms preprocess, 4.4ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "\n",
      "Epoch  40/120\n",
      "box_loss  0.5412\n",
      "cls_loss  0.5818\n",
      "dfl_loss  0.9064\n",
      "Box(P  0.940\n",
      "R  0.817\n",
      "mAP50  0.94\n",
      "mAP50-95)  0.73\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 295.293.7 MB/s, size: 25.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.942      0.816      0.942      0.746\n",
      "Speed: 0.7ms preprocess, 4.1ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "\n",
      "Epoch  41/120\n",
      "box_loss  0.5197\n",
      "cls_loss  0.6067\n",
      "dfl_loss  0.8994\n",
      "Box(P  0.942\n",
      "R  0.816\n",
      "mAP50  0.94\n",
      "mAP50-95)  0.75\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 278.348.2 MB/s, size: 21.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.932       0.85       0.94      0.743\n",
      "Speed: 0.7ms preprocess, 4.6ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "\n",
      "Epoch  42/120\n",
      "box_loss  0.5619\n",
      "cls_loss  0.6188\n",
      "dfl_loss  0.9059\n",
      "Box(P  0.932\n",
      "R  0.850\n",
      "mAP50  0.94\n",
      "mAP50-95)  0.74\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 261.266.4 MB/s, size: 25.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.932      0.807      0.939      0.737\n",
      "Speed: 0.7ms preprocess, 4.1ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "\n",
      "Epoch  43/120\n",
      "box_loss  0.5262\n",
      "cls_loss  0.5646\n",
      "dfl_loss  0.8963\n",
      "Box(P  0.932\n",
      "R  0.807\n",
      "mAP50  0.94\n",
      "mAP50-95)  0.74\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 283.737.6 MB/s, size: 23.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.873      0.838      0.942      0.761\n",
      "Speed: 0.7ms preprocess, 4.2ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "\n",
      "Epoch  44/120\n",
      "box_loss  0.5165\n",
      "cls_loss  0.5771\n",
      "dfl_loss  0.8869\n",
      "Box(P  0.873\n",
      "R  0.838\n",
      "mAP50  0.94\n",
      "mAP50-95)  0.76\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 255.150.0 MB/s, size: 24.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.9Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.921      0.804      0.948      0.771\n",
      "Speed: 0.7ms preprocess, 4.3ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "\n",
      "Epoch  45/120\n",
      "box_loss  0.4968\n",
      "cls_loss  0.5727\n",
      "dfl_loss  0.8765\n",
      "Box(P  0.921\n",
      "R  0.804\n",
      "mAP50  0.95\n",
      "mAP50-95)  0.77\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 266.342.3 MB/s, size: 23.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.9Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.833      0.853      0.948      0.763\n",
      "Speed: 0.7ms preprocess, 4.7ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "\n",
      "Epoch  46/120\n",
      "box_loss  0.5081\n",
      "cls_loss  0.6016\n",
      "dfl_loss  0.8847\n",
      "Box(P  0.833\n",
      "R  0.853\n",
      "mAP50  0.95\n",
      "mAP50-95)  0.76\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 214.652.8 MB/s, size: 24.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.5s0.2ss\n",
      "                   all         61        112       0.95      0.821      0.945      0.757\n",
      "Speed: 0.7ms preprocess, 5.0ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "\n",
      "Epoch  47/120\n",
      "box_loss  0.5218\n",
      "cls_loss  0.5933\n",
      "dfl_loss  0.9022\n",
      "Box(P  0.950\n",
      "R  0.821\n",
      "mAP50  0.95\n",
      "mAP50-95)  0.76\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 229.552.5 MB/s, size: 25.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.1Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.947      0.855      0.952      0.761\n",
      "Speed: 0.6ms preprocess, 4.6ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "\n",
      "Epoch  48/120\n",
      "box_loss  0.4704\n",
      "cls_loss  0.5533\n",
      "dfl_loss  0.8722\n",
      "Box(P  0.947\n",
      "R  0.855\n",
      "mAP50  0.95\n",
      "mAP50-95)  0.76\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 255.845.3 MB/s, size: 24.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.3s0.2ss\n",
      "                   all         61        112      0.898      0.855      0.949      0.755\n",
      "Speed: 0.6ms preprocess, 4.5ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "\n",
      "Epoch  49/120\n",
      "box_loss  0.4873\n",
      "cls_loss  0.5776\n",
      "dfl_loss  0.8877\n",
      "Box(P  0.898\n",
      "R  0.855\n",
      "mAP50  0.95\n",
      "mAP50-95)  0.76\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 324.4104.0 MB/s, size: 25.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.5s0.2ss\n",
      "                   all         61        112       0.89      0.846       0.95      0.752\n",
      "Speed: 0.8ms preprocess, 4.6ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "\n",
      "Epoch  50/120\n",
      "box_loss  0.4714\n",
      "cls_loss  0.5511\n",
      "dfl_loss  0.8749\n",
      "Box(P  0.890\n",
      "R  0.846\n",
      "mAP50  0.95\n",
      "mAP50-95)  0.75\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 308.347.0 MB/s, size: 24.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.918      0.848      0.958      0.778\n",
      "Speed: 0.7ms preprocess, 5.2ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "\n",
      "Epoch  51/120\n",
      "box_loss  0.4763\n",
      "cls_loss  0.5429\n",
      "dfl_loss  0.8712\n",
      "Box(P  0.918\n",
      "R  0.848\n",
      "mAP50  0.96\n",
      "mAP50-95)  0.78\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 303.735.2 MB/s, size: 23.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.5s0.2ss\n",
      "                   all         61        112      0.938      0.862      0.948      0.772\n",
      "Speed: 0.7ms preprocess, 4.1ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "\n",
      "Epoch  52/120\n",
      "box_loss  0.5048\n",
      "cls_loss  0.5594\n",
      "dfl_loss  0.8856\n",
      "Box(P  0.938\n",
      "R  0.862\n",
      "mAP50  0.95\n",
      "mAP50-95)  0.77\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 285.141.2 MB/s, size: 22.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.887      0.877      0.952      0.772\n",
      "Speed: 0.6ms preprocess, 4.4ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "\n",
      "Epoch  53/120\n",
      "box_loss  0.5071\n",
      "cls_loss  0.5525\n",
      "dfl_loss  0.8866\n",
      "Box(P  0.887\n",
      "R  0.877\n",
      "mAP50  0.95\n",
      "mAP50-95)  0.77\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 337.278.3 MB/s, size: 29.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.5s0.2ss\n",
      "                   all         61        112      0.939      0.866      0.952      0.771\n",
      "Speed: 0.7ms preprocess, 4.6ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "\n",
      "Epoch  54/120\n",
      "box_loss  0.4553\n",
      "cls_loss  0.5265\n",
      "dfl_loss  0.8715\n",
      "Box(P  0.939\n",
      "R  0.866\n",
      "mAP50  0.95\n",
      "mAP50-95)  0.77\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 250.962.7 MB/s, size: 26.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 40.5Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.4it/s 2.9s0.3ss\n",
      "                   all         61        112      0.953      0.861      0.958       0.78\n",
      "Speed: 0.8ms preprocess, 5.0ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
      "\n",
      "Epoch  55/120\n",
      "box_loss  0.4550\n",
      "cls_loss  0.5222\n",
      "dfl_loss  0.8756\n",
      "Box(P  0.953\n",
      "R  0.861\n",
      "mAP50  0.96\n",
      "mAP50-95)  0.78\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 269.838.1 MB/s, size: 25.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.4it/s 2.9s0.3ss\n",
      "                   all         61        112      0.901      0.883      0.956       0.78\n",
      "Speed: 0.8ms preprocess, 5.3ms inference, 0.0ms loss, 1.6ms postprocess per image\n",
      "\n",
      "Epoch  56/120\n",
      "box_loss  0.4726\n",
      "cls_loss  0.5392\n",
      "dfl_loss  0.8686\n",
      "Box(P  0.901\n",
      "R  0.883\n",
      "mAP50  0.96\n",
      "mAP50-95)  0.78\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 245.424.6 MB/s, size: 22.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.4it/s 2.9s0.3ss\n",
      "                   all         61        112      0.922      0.866      0.962      0.785\n",
      "Speed: 0.7ms preprocess, 5.0ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "\n",
      "Epoch  57/120\n",
      "box_loss  0.4471\n",
      "cls_loss  0.5116\n",
      "dfl_loss  0.8654\n",
      "Box(P  0.922\n",
      "R  0.866\n",
      "mAP50  0.96\n",
      "mAP50-95)  0.78\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 252.930.5 MB/s, size: 22.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.9Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.4it/s 2.8s0.3ss\n",
      "                   all         61        112      0.942       0.85      0.961      0.776\n",
      "Speed: 0.7ms preprocess, 5.5ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "\n",
      "Epoch  58/120\n",
      "box_loss  0.4607\n",
      "cls_loss  0.5474\n",
      "dfl_loss  0.8640\n",
      "Box(P  0.942\n",
      "R  0.850\n",
      "mAP50  0.96\n",
      "mAP50-95)  0.78\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 244.843.2 MB/s, size: 22.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.4it/s 2.8s0.3ss\n",
      "                   all         61        112      0.912      0.884      0.959      0.773\n",
      "Speed: 0.7ms preprocess, 5.2ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
      "\n",
      "Epoch  59/120\n",
      "box_loss  0.4783\n",
      "cls_loss  0.5382\n",
      "dfl_loss  0.8804\n",
      "Box(P  0.912\n",
      "R  0.884\n",
      "mAP50  0.96\n",
      "mAP50-95)  0.77\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 198.330.6 MB/s, size: 24.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.4it/s 2.8s0.2ss\n",
      "                   all         61        112      0.914      0.885      0.962      0.791\n",
      "Speed: 0.7ms preprocess, 4.5ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "\n",
      "Epoch  60/120\n",
      "box_loss  0.4773\n",
      "cls_loss  0.5265\n",
      "dfl_loss  0.8855\n",
      "Box(P  0.914\n",
      "R  0.885\n",
      "mAP50  0.96\n",
      "mAP50-95)  0.79\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 242.635.7 MB/s, size: 21.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.4it/s 2.8s0.2ss\n",
      "                   all         61        112      0.889      0.882      0.961      0.781\n",
      "Speed: 0.7ms preprocess, 4.9ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "\n",
      "Epoch  61/120\n",
      "box_loss  0.4717\n",
      "cls_loss  0.5406\n",
      "dfl_loss  0.8831\n",
      "Box(P  0.889\n",
      "R  0.882\n",
      "mAP50  0.96\n",
      "mAP50-95)  0.78\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 244.534.2 MB/s, size: 22.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.5it/s 2.7s0.8ss\n",
      "                   all         61        112      0.845      0.879      0.953      0.777\n",
      "Speed: 0.7ms preprocess, 3.7ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "\n",
      "Epoch  62/120\n",
      "box_loss  0.4385\n",
      "cls_loss  0.5156\n",
      "dfl_loss  0.8646\n",
      "Box(P  0.845\n",
      "R  0.879\n",
      "mAP50  0.95\n",
      "mAP50-95)  0.78\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 252.469.2 MB/s, size: 29.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.4it/s 2.9s0.3ss\n",
      "                   all         61        112      0.917      0.856      0.959      0.791\n",
      "Speed: 0.7ms preprocess, 4.7ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "\n",
      "Epoch  63/120\n",
      "box_loss  0.4363\n",
      "cls_loss  0.5277\n",
      "dfl_loss  0.8641\n",
      "Box(P  0.917\n",
      "R  0.856\n",
      "mAP50  0.96\n",
      "mAP50-95)  0.79\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 288.151.4 MB/s, size: 26.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.4it/s 2.9s0.2ss\n",
      "                   all         61        112      0.956      0.851      0.962      0.792\n",
      "Speed: 0.7ms preprocess, 4.8ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "\n",
      "Epoch  64/120\n",
      "box_loss  0.4457\n",
      "cls_loss  0.5197\n",
      "dfl_loss  0.8650\n",
      "Box(P  0.956\n",
      "R  0.851\n",
      "mAP50  0.96\n",
      "mAP50-95)  0.79\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 217.358.8 MB/s, size: 24.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.4it/s 2.9s0.3ss\n",
      "                   all         61        112      0.943      0.886      0.965      0.778\n",
      "Speed: 0.7ms preprocess, 5.0ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "\n",
      "Epoch  65/120\n",
      "box_loss  0.4089\n",
      "cls_loss  0.4927\n",
      "dfl_loss  0.8478\n",
      "Box(P  0.943\n",
      "R  0.886\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.78\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 276.824.8 MB/s, size: 23.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.9Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.6s0.3ss\n",
      "                   all         61        112      0.944      0.885      0.964      0.777\n",
      "Speed: 0.7ms preprocess, 4.4ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "\n",
      "Epoch  66/120\n",
      "box_loss  0.4333\n",
      "cls_loss  0.5106\n",
      "dfl_loss  0.8627\n",
      "Box(P  0.944\n",
      "R  0.885\n",
      "mAP50  0.96\n",
      "mAP50-95)  0.78\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 305.8100.8 MB/s, size: 27.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.913       0.92      0.967      0.774\n",
      "Speed: 0.6ms preprocess, 4.2ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "\n",
      "Epoch  67/120\n",
      "box_loss  0.4570\n",
      "cls_loss  0.5203\n",
      "dfl_loss  0.8667\n",
      "Box(P  0.913\n",
      "R  0.920\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.77\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 364.688.2 MB/s, size: 29.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.914      0.919      0.969      0.781\n",
      "Speed: 0.7ms preprocess, 4.4ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "\n",
      "Epoch  68/120\n",
      "box_loss  0.4398\n",
      "cls_loss  0.4896\n",
      "dfl_loss  0.8666\n",
      "Box(P  0.914\n",
      "R  0.919\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.78\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 255.826.0 MB/s, size: 22.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.3s0.3ss\n",
      "                   all         61        112      0.931      0.923      0.971      0.786\n",
      "Speed: 0.7ms preprocess, 4.8ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "\n",
      "Epoch  69/120\n",
      "box_loss  0.4262\n",
      "cls_loss  0.4781\n",
      "dfl_loss  0.8580\n",
      "Box(P  0.931\n",
      "R  0.923\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.79\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 294.228.1 MB/s, size: 23.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.8Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.5s0.3ss\n",
      "                   all         61        112      0.924      0.896      0.968      0.782\n",
      "Speed: 0.8ms preprocess, 5.4ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "\n",
      "Epoch  70/120\n",
      "box_loss  0.4093\n",
      "cls_loss  0.4792\n",
      "dfl_loss  0.8501\n",
      "Box(P  0.924\n",
      "R  0.896\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.78\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 305.857.7 MB/s, size: 24.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.905      0.883      0.966      0.792\n",
      "Speed: 0.6ms preprocess, 4.5ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "\n",
      "Epoch  71/120\n",
      "box_loss  0.4273\n",
      "cls_loss  0.4934\n",
      "dfl_loss  0.8640\n",
      "Box(P  0.905\n",
      "R  0.883\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.79\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 261.728.2 MB/s, size: 23.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.5s0.2ss\n",
      "                   all         61        112      0.961      0.877      0.965      0.779\n",
      "Speed: 0.7ms preprocess, 5.0ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "\n",
      "Epoch  72/120\n",
      "box_loss  0.4908\n",
      "cls_loss  0.5249\n",
      "dfl_loss  0.8931\n",
      "Box(P  0.961\n",
      "R  0.877\n",
      "mAP50  0.96\n",
      "mAP50-95)  0.78\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 250.761.3 MB/s, size: 25.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.4s0.2ss\n",
      "                   all         61        112       0.96      0.873      0.964       0.79\n",
      "Speed: 0.7ms preprocess, 5.2ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "\n",
      "Epoch  73/120\n",
      "box_loss  0.4143\n",
      "cls_loss  0.4891\n",
      "dfl_loss  0.8580\n",
      "Box(P  0.960\n",
      "R  0.873\n",
      "mAP50  0.96\n",
      "mAP50-95)  0.79\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 269.537.4 MB/s, size: 22.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.1Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.3s0.2ss\n",
      "                   all         61        112      0.933      0.893      0.965      0.791\n",
      "Speed: 0.7ms preprocess, 3.9ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "\n",
      "Epoch  74/120\n",
      "box_loss  0.4348\n",
      "cls_loss  0.4970\n",
      "dfl_loss  0.8665\n",
      "Box(P  0.933\n",
      "R  0.893\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.79\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 253.855.6 MB/s, size: 22.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.9Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.3s0.2ss\n",
      "                   all         61        112      0.939      0.852      0.967      0.796\n",
      "Speed: 0.7ms preprocess, 4.1ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "\n",
      "Epoch  75/120\n",
      "box_loss  0.4239\n",
      "cls_loss  0.4945\n",
      "dfl_loss  0.8592\n",
      "Box(P  0.939\n",
      "R  0.852\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.80\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 314.470.2 MB/s, size: 24.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.937      0.885      0.959      0.794\n",
      "Speed: 0.7ms preprocess, 5.0ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "\n",
      "Epoch  76/120\n",
      "box_loss  0.4182\n",
      "cls_loss  0.5028\n",
      "dfl_loss  0.8666\n",
      "Box(P  0.937\n",
      "R  0.885\n",
      "mAP50  0.96\n",
      "mAP50-95)  0.79\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 311.760.0 MB/s, size: 25.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.8Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.968      0.885      0.968       0.79\n",
      "Speed: 0.7ms preprocess, 4.4ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "\n",
      "Epoch  77/120\n",
      "box_loss  0.4206\n",
      "cls_loss  0.4850\n",
      "dfl_loss  0.8594\n",
      "Box(P  0.968\n",
      "R  0.885\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.79\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 300.950.1 MB/s, size: 26.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 59.2Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.926      0.901      0.971       0.79\n",
      "Speed: 0.7ms preprocess, 4.0ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "\n",
      "Epoch  78/120\n",
      "box_loss  0.4297\n",
      "cls_loss  0.4881\n",
      "dfl_loss  0.8585\n",
      "Box(P  0.926\n",
      "R  0.901\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.79\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 376.168.5 MB/s, size: 30.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.7Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.959      0.885      0.969      0.791\n",
      "Speed: 0.6ms preprocess, 4.5ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "\n",
      "Epoch  79/120\n",
      "box_loss  0.3903\n",
      "cls_loss  0.4825\n",
      "dfl_loss  0.8432\n",
      "Box(P  0.959\n",
      "R  0.885\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.79\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 216.018.3 MB/s, size: 21.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.906      0.885      0.961       0.79\n",
      "Speed: 0.6ms preprocess, 4.7ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "\n",
      "Epoch  80/120\n",
      "box_loss  0.3996\n",
      "cls_loss  0.4757\n",
      "dfl_loss  0.8591\n",
      "Box(P  0.906\n",
      "R  0.885\n",
      "mAP50  0.96\n",
      "mAP50-95)  0.79\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 294.046.4 MB/s, size: 24.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.9Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.4s0.3ss\n",
      "                   all         61        112      0.955      0.882      0.963       0.78\n",
      "Speed: 0.6ms preprocess, 4.6ms inference, 0.0ms loss, 1.6ms postprocess per image\n",
      "\n",
      "Epoch  81/120\n",
      "box_loss  0.4462\n",
      "cls_loss  0.5041\n",
      "dfl_loss  0.8708\n",
      "Box(P  0.955\n",
      "R  0.882\n",
      "mAP50  0.96\n",
      "mAP50-95)  0.78\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 241.234.7 MB/s, size: 23.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.8Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.947      0.914       0.97      0.796\n",
      "Speed: 0.7ms preprocess, 4.4ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "\n",
      "Epoch  82/120\n",
      "box_loss  0.3978\n",
      "cls_loss  0.4735\n",
      "dfl_loss  0.8519\n",
      "Box(P  0.947\n",
      "R  0.914\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.80\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 209.933.5 MB/s, size: 20.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.8Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.4s0.2ss\n",
      "                   all         61        112      0.948      0.888      0.966      0.794\n",
      "Speed: 0.6ms preprocess, 4.6ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "\n",
      "Epoch  83/120\n",
      "box_loss  0.4023\n",
      "cls_loss  0.4830\n",
      "dfl_loss  0.8504\n",
      "Box(P  0.948\n",
      "R  0.888\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.79\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 233.634.2 MB/s, size: 21.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.9Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.3s0.2ss\n",
      "                   all         61        112      0.947      0.879      0.967      0.803\n",
      "Speed: 0.8ms preprocess, 4.7ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "\n",
      "Epoch  84/120\n",
      "box_loss  0.4270\n",
      "cls_loss  0.4906\n",
      "dfl_loss  0.8808\n",
      "Box(P  0.947\n",
      "R  0.879\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.80\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 257.653.0 MB/s, size: 24.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.972      0.899      0.965        0.8\n",
      "Speed: 0.6ms preprocess, 4.5ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "\n",
      "Epoch  85/120\n",
      "box_loss  0.3805\n",
      "cls_loss  0.4658\n",
      "dfl_loss  0.8370\n",
      "Box(P  0.972\n",
      "R  0.899\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.80\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 304.151.0 MB/s, size: 24.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.952      0.887      0.967      0.802\n",
      "Speed: 0.6ms preprocess, 4.7ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "\n",
      "Epoch  86/120\n",
      "box_loss  0.4317\n",
      "cls_loss  0.4959\n",
      "dfl_loss  0.8665\n",
      "Box(P  0.952\n",
      "R  0.887\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.80\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 264.448.7 MB/s, size: 24.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.937      0.865      0.962      0.793\n",
      "Speed: 0.6ms preprocess, 5.1ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "\n",
      "Epoch  87/120\n",
      "box_loss  0.4283\n",
      "cls_loss  0.4908\n",
      "dfl_loss  0.8610\n",
      "Box(P  0.937\n",
      "R  0.865\n",
      "mAP50  0.96\n",
      "mAP50-95)  0.79\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 261.339.5 MB/s, size: 24.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.946      0.878      0.964      0.805\n",
      "Speed: 0.6ms preprocess, 4.1ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "\n",
      "Epoch  88/120\n",
      "box_loss  0.3924\n",
      "cls_loss  0.4498\n",
      "dfl_loss  0.8487\n",
      "Box(P  0.946\n",
      "R  0.878\n",
      "mAP50  0.96\n",
      "mAP50-95)  0.80\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 316.355.3 MB/s, size: 25.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.3s0.2ss\n",
      "                   all         61        112       0.95      0.872      0.964      0.805\n",
      "Speed: 0.6ms preprocess, 4.0ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "\n",
      "Epoch  89/120\n",
      "box_loss  0.4155\n",
      "cls_loss  0.4825\n",
      "dfl_loss  0.8522\n",
      "Box(P  0.950\n",
      "R  0.872\n",
      "mAP50  0.96\n",
      "mAP50-95)  0.80\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 272.040.8 MB/s, size: 25.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.3Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.952      0.885      0.966      0.805\n",
      "Speed: 0.8ms preprocess, 4.8ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "\n",
      "Epoch  90/120\n",
      "box_loss  0.3756\n",
      "cls_loss  0.4435\n",
      "dfl_loss  0.8447\n",
      "Box(P  0.952\n",
      "R  0.885\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.81\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 268.337.3 MB/s, size: 25.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.945       0.89      0.967        0.8\n",
      "Speed: 0.6ms preprocess, 4.9ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "\n",
      "Epoch  91/120\n",
      "box_loss  0.3684\n",
      "cls_loss  0.4470\n",
      "dfl_loss  0.8382\n",
      "Box(P  0.945\n",
      "R  0.890\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.80\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 299.952.3 MB/s, size: 23.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.9Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.3s0.2ss\n",
      "                   all         61        112      0.961       0.89      0.969      0.809\n",
      "Speed: 0.7ms preprocess, 4.6ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "\n",
      "Epoch  92/120\n",
      "box_loss  0.3860\n",
      "cls_loss  0.4566\n",
      "dfl_loss  0.8426\n",
      "Box(P  0.961\n",
      "R  0.890\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.81\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 263.543.1 MB/s, size: 24.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.7Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.3s0.2ss\n",
      "                   all         61        112      0.952       0.89      0.969      0.804\n",
      "Speed: 0.7ms preprocess, 4.4ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "\n",
      "Epoch  93/120\n",
      "box_loss  0.3304\n",
      "cls_loss  0.4325\n",
      "dfl_loss  0.8281\n",
      "Box(P  0.952\n",
      "R  0.890\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.80\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 278.324.2 MB/s, size: 22.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.946      0.896       0.97      0.809\n",
      "Speed: 0.8ms preprocess, 4.7ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "\n",
      "Epoch  94/120\n",
      "box_loss  0.3729\n",
      "cls_loss  0.4499\n",
      "dfl_loss  0.8517\n",
      "Box(P  0.946\n",
      "R  0.896\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.81\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 310.134.0 MB/s, size: 24.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.4s0.2ss\n",
      "                   all         61        112      0.949      0.901      0.971      0.806\n",
      "Speed: 0.6ms preprocess, 4.0ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "\n",
      "Epoch  95/120\n",
      "box_loss  0.3884\n",
      "cls_loss  0.4549\n",
      "dfl_loss  0.8536\n",
      "Box(P  0.949\n",
      "R  0.901\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.81\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 279.453.4 MB/s, size: 25.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.2ss\n",
      "                   all         61        112      0.959      0.895      0.967      0.803\n",
      "Speed: 0.6ms preprocess, 4.0ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "\n",
      "Epoch  96/120\n",
      "box_loss  0.3627\n",
      "cls_loss  0.4536\n",
      "dfl_loss  0.8357\n",
      "Box(P  0.959\n",
      "R  0.895\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.80\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 337.948.6 MB/s, size: 26.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.5it/s 2.6s0.3ss\n",
      "                   all         61        112      0.973      0.889       0.97      0.804\n",
      "Speed: 0.7ms preprocess, 5.2ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "\n",
      "Epoch  97/120\n",
      "box_loss  0.3775\n",
      "cls_loss  0.4316\n",
      "dfl_loss  0.8512\n",
      "Box(P  0.973\n",
      "R  0.889\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.80\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 302.249.6 MB/s, size: 24.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.5it/s 2.6s0.2ss\n",
      "                   all         61        112      0.967      0.874      0.972      0.809\n",
      "Speed: 0.6ms preprocess, 4.9ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "\n",
      "Epoch  98/120\n",
      "box_loss  0.4085\n",
      "cls_loss  0.4758\n",
      "dfl_loss  0.8669\n",
      "Box(P  0.967\n",
      "R  0.874\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.81\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 206.147.8 MB/s, size: 24.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.5s0.2ss\n",
      "                   all         61        112      0.941      0.896      0.971      0.816\n",
      "Speed: 0.6ms preprocess, 5.0ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "\n",
      "Epoch  99/120\n",
      "box_loss  0.3557\n",
      "cls_loss  0.4290\n",
      "dfl_loss  0.8413\n",
      "Box(P  0.941\n",
      "R  0.896\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.82\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 251.829.7 MB/s, size: 23.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.5s0.3ss\n",
      "                   all         61        112       0.96      0.896      0.971      0.811\n",
      "Speed: 0.6ms preprocess, 5.2ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "\n",
      "Epoch  100/120\n",
      "box_loss  0.3587\n",
      "cls_loss  0.4403\n",
      "dfl_loss  0.8394\n",
      "Box(P  0.960\n",
      "R  0.896\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.81\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 247.916.2 MB/s, size: 22.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.5it/s 2.7s0.3ss\n",
      "                   all         61        112      0.968       0.89       0.97      0.823\n",
      "Speed: 0.7ms preprocess, 5.2ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "\n",
      "Epoch  101/120\n",
      "box_loss  0.3560\n",
      "cls_loss  0.4308\n",
      "dfl_loss  0.8402\n",
      "Box(P  0.968\n",
      "R  0.890\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.82\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 299.059.5 MB/s, size: 23.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.5it/s 2.6s0.2ss\n",
      "                   all         61        112      0.963      0.896       0.97      0.821\n",
      "Speed: 0.8ms preprocess, 5.1ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "\n",
      "Epoch  102/120\n",
      "box_loss  0.3621\n",
      "cls_loss  0.4443\n",
      "dfl_loss  0.8422\n",
      "Box(P  0.963\n",
      "R  0.896\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.82\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 258.948.1 MB/s, size: 25.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.5it/s 2.7s0.3ss\n",
      "                   all         61        112       0.95      0.901       0.97       0.82\n",
      "Speed: 0.7ms preprocess, 4.7ms inference, 0.0ms loss, 1.6ms postprocess per image\n",
      "\n",
      "Epoch  103/120\n",
      "box_loss  0.3341\n",
      "cls_loss  0.4267\n",
      "dfl_loss  0.8344\n",
      "Box(P  0.950\n",
      "R  0.901\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.82\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 292.170.4 MB/s, size: 23.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.3s0.2ss\n",
      "                   all         61        112      0.968      0.889      0.973      0.823\n",
      "Speed: 0.7ms preprocess, 4.5ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "\n",
      "Epoch  104/120\n",
      "box_loss  0.3581\n",
      "cls_loss  0.4205\n",
      "dfl_loss  0.8441\n",
      "Box(P  0.968\n",
      "R  0.889\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.82\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 253.876.0 MB/s, size: 23.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.5s0.2ss\n",
      "                   all         61        112      0.964      0.882      0.973      0.824\n",
      "Speed: 0.7ms preprocess, 4.4ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "\n",
      "Epoch  105/120\n",
      "box_loss  0.3497\n",
      "cls_loss  0.4311\n",
      "dfl_loss  0.8433\n",
      "Box(P  0.964\n",
      "R  0.882\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.82\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 247.642.2 MB/s, size: 24.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.9Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.5it/s 2.7s0.3ss\n",
      "                   all         61        112      0.973      0.881      0.971      0.822\n",
      "Speed: 0.8ms preprocess, 4.7ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "\n",
      "Epoch  106/120\n",
      "box_loss  0.3511\n",
      "cls_loss  0.4329\n",
      "dfl_loss  0.8311\n",
      "Box(P  0.973\n",
      "R  0.881\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.82\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 302.360.3 MB/s, size: 24.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.9Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.4it/s 2.8s0.3ss\n",
      "                   all         61        112      0.957      0.884       0.97      0.818\n",
      "Speed: 0.7ms preprocess, 4.9ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "\n",
      "Epoch  107/120\n",
      "box_loss  0.3609\n",
      "cls_loss  0.4292\n",
      "dfl_loss  0.8434\n",
      "Box(P  0.957\n",
      "R  0.884\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.82\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 322.840.1 MB/s, size: 25.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.4it/s 2.8s0.3ss\n",
      "                   all         61        112      0.958      0.891      0.972      0.817\n",
      "Speed: 0.8ms preprocess, 4.8ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
      "\n",
      "Epoch  108/120\n",
      "box_loss  0.3664\n",
      "cls_loss  0.4499\n",
      "dfl_loss  0.8434\n",
      "Box(P  0.958\n",
      "R  0.891\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.82\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 283.761.1 MB/s, size: 22.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.8Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.5it/s 2.6s0.3ss\n",
      "                   all         61        112      0.962       0.89      0.971      0.817\n",
      "Speed: 0.7ms preprocess, 4.8ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
      "\n",
      "Epoch  109/120\n",
      "box_loss  0.3549\n",
      "cls_loss  0.4375\n",
      "dfl_loss  0.8418\n",
      "Box(P  0.962\n",
      "R  0.890\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.82\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 230.339.3 MB/s, size: 26.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.1Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.5it/s 2.7s0.3ss\n",
      "                   all         61        112      0.942       0.89      0.968      0.812\n",
      "Speed: 0.8ms preprocess, 5.5ms inference, 0.0ms loss, 1.6ms postprocess per image\n",
      "\n",
      "Epoch  110/120\n",
      "box_loss  0.3921\n",
      "cls_loss  0.4675\n",
      "dfl_loss  0.8511\n",
      "Box(P  0.942\n",
      "R  0.890\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.81\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 311.049.4 MB/s, size: 25.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.4s1.0ss\n",
      "                   all         61        112      0.933      0.896      0.969      0.809\n",
      "Speed: 0.6ms preprocess, 4.0ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "\n",
      "Epoch  111/120\n",
      "box_loss  0.3848\n",
      "cls_loss  0.4767\n",
      "dfl_loss  0.8503\n",
      "Box(P  0.933\n",
      "R  0.896\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.81\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 274.517.2 MB/s, size: 22.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.7it/s 2.4s0.3ss\n",
      "                   all         61        112       0.95      0.893      0.969      0.812\n",
      "Speed: 0.8ms preprocess, 4.5ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "\n",
      "Epoch  112/120\n",
      "box_loss  0.3823\n",
      "cls_loss  0.4381\n",
      "dfl_loss  0.8556\n",
      "Box(P  0.950\n",
      "R  0.893\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.81\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 336.385.2 MB/s, size: 28.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.5s0.3ss\n",
      "                   all         61        112      0.953      0.896      0.969      0.816\n",
      "Speed: 0.7ms preprocess, 4.0ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "\n",
      "Epoch  113/120\n",
      "box_loss  0.3978\n",
      "cls_loss  0.4453\n",
      "dfl_loss  0.8568\n",
      "Box(P  0.953\n",
      "R  0.896\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.82\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 258.941.8 MB/s, size: 24.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.9Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.6s0.2ss\n",
      "                   all         61        112      0.949      0.892      0.967      0.811\n",
      "Speed: 0.7ms preprocess, 4.4ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "\n",
      "Epoch  114/120\n",
      "box_loss  0.3349\n",
      "cls_loss  0.4298\n",
      "dfl_loss  0.8352\n",
      "Box(P  0.949\n",
      "R  0.892\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.81\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 246.630.2 MB/s, size: 23.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.5s0.2ss\n",
      "                   all         61        112      0.955      0.896      0.969      0.813\n",
      "Speed: 0.7ms preprocess, 5.0ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "\n",
      "Epoch  115/120\n",
      "box_loss  0.3137\n",
      "cls_loss  0.4151\n",
      "dfl_loss  0.8247\n",
      "Box(P  0.955\n",
      "R  0.896\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.81\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 318.357.1 MB/s, size: 25.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.5s0.3ss\n",
      "                   all         61        112      0.952      0.896      0.968      0.812\n",
      "Speed: 0.9ms preprocess, 4.5ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "\n",
      "Epoch  116/120\n",
      "box_loss  0.3982\n",
      "cls_loss  0.4607\n",
      "dfl_loss  0.8557\n",
      "Box(P  0.952\n",
      "R  0.896\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.81\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 270.150.4 MB/s, size: 23.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.5it/s 2.7s0.3ss\n",
      "                   all         61        112      0.955      0.896      0.971      0.812\n",
      "Speed: 0.8ms preprocess, 4.8ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
      "\n",
      "Epoch  117/120\n",
      "box_loss  0.3184\n",
      "cls_loss  0.4195\n",
      "dfl_loss  0.8229\n",
      "Box(P  0.955\n",
      "R  0.896\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.81\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 247.725.1 MB/s, size: 24.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 61.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.5s0.3ss\n",
      "                   all         61        112      0.938      0.896      0.969       0.81\n",
      "Speed: 0.8ms preprocess, 4.5ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "\n",
      "Epoch  118/120\n",
      "box_loss  0.3552\n",
      "cls_loss  0.4248\n",
      "dfl_loss  0.8547\n",
      "Box(P  0.938\n",
      "R  0.896\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.81\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 255.756.0 MB/s, size: 24.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.5s0.2ss\n",
      "                   all         61        112      0.946      0.892       0.97      0.821\n",
      "Speed: 0.7ms preprocess, 4.8ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "\n",
      "Epoch  119/120\n",
      "box_loss  0.3122\n",
      "cls_loss  0.4175\n",
      "dfl_loss  0.8223\n",
      "Box(P  0.946\n",
      "R  0.892\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.82\n",
      "Ultralytics 8.3.248  Python-3.11.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4060, 8187MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 303.322.0 MB/s, size: 29.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pichau\\Desktop\\Programacao\\FruitNAI\\datasets\\fruitninja_yolo\\labels\\val.cache... 61 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 61/61 60.9Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.5it/s 2.7s0.3ss\n",
      "                   all         61        112      0.938      0.896       0.97      0.814\n",
      "Speed: 0.8ms preprocess, 4.9ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "\n",
      "Epoch  120/120\n",
      "box_loss  0.3499\n",
      "cls_loss  0.4388\n",
      "dfl_loss  0.8391\n",
      "Box(P  0.938\n",
      "R  0.896\n",
      "mAP50  0.97\n",
      "mAP50-95)  0.81\n",
      "Salvo em: ../models/runs/fruitninja_yolo_manual/yolo_state_dict.pt\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad083baa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
